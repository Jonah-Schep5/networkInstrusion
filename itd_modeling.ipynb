{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1408df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jonahs23/networkInstrusion/.env\n",
      "User:  jonahs23\n",
      "Database:  postgresql://jonahs23:DataSci23@ads1.datasci.vt.edu:5432/ads_db5\n"
     ]
    }
   ],
   "source": [
    "%run  \"./env_setup.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac092697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonahs23/networkInstrusion/PostgresAgent.py:90: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "table = \"network_traffic_history_itd\"\n",
    "sql = f\"\"\"select\n",
    "dest_port,\n",
    "protocol,\n",
    "duration,\n",
    "packets,\n",
    "bytes,\n",
    "bytes_per_packet,\n",
    "tcp_flags,\n",
    "packet_size_variance,\n",
    "connection_frequency,\n",
    "off_hours,\n",
    "is_weekend,\n",
    "hour_of_day,\n",
    "is_internal_source,\n",
    "is_internal_dest,\n",
    "is_itd,\n",
    "time_period,\n",
    "bytes_percentile,\n",
    "packets_percentile,\n",
    "duration_percentile,\n",
    "bpp_percentile,\n",
    "service\n",
    "\n",
    "from {username}.{table} pd\n",
    "\"\"\"\n",
    "\n",
    "df = agent.execute_dml(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb264653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70c5edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Check class distribution before SMOTE\\nfrom collections import Counter\\nprint(\"Class distribution before SMOTE:\")\\nprint(f\"Total samples: {len(y):,}\")\\nprint(f\"Class 0 (Normal): {sum(y == 0):,}\")\\nprint(f\"Class 1 (ITD): {sum(y == 1):,}\")\\nprint(f\"Imbalance ratio: {sum(y == 0) / sum(y == 1):.2f}:1\")\\nprint(f\"\\nTarget distribution:\\n{Counter(y)}\")\\n\\n# Apply SMOTE to upsample the minority class to 2:1 ratio (Normal:ITD)\\nfrom imblearn.over_sampling import SMOTE\\nprint(\"\\nApplying SMOTE upsampling to 2:1 ratio (Normal:ITD)...\")\\n\\n# Calculate target ratio: we want Class 0 : Class 1 = 2:1\\n# So Class 1 should be Class 0 / 2\\nminority_target = sum(y == 0) // 10\\n\\nsmote = SMOTE(random_state=42, sampling_strategy={1: minority_target}, k_neighbors=5)\\nX_resampled, y_resampled = smote.fit_resample(X, y)\\n\\n# Check class distribution after SMOTE\\nprint(\"\\nClass distribution after SMOTE:\")\\nprint(f\"Total samples: {len(y_resampled):,}\")\\nprint(f\"Class 0 (Normal): {sum(y_resampled == 0):,}\")\\nprint(f\"Class 1 (ITD): {sum(y_resampled == 1):,}\")\\nprint(f\"New ratio: {sum(y_resampled == 0) / sum(y_resampled == 1):.2f}:1\")\\nprint(f\"\\nTarget distribution:\\n{Counter(y_resampled)}\")\\n\\n# Calculate expected NIR after SMOTE\\nexpected_nir = sum(y_resampled == 0) / len(y_resampled)\\nprint(f\"\\nExpected No Information Rate (NIR) after SMOTE: {expected_nir:.4f}\")\\n\\n# Update X and y to use resampled data\\nX = pd.DataFrame(X_resampled, columns=X.columns)\\ny = pd.Series(y_resampled, name=\\'is_itd\\')\\n\\nprint(\"\\nSMOTE upsampling completed successfully!\")\\nprint(f\"Generated {sum(y_resampled == 1) - sum(df[\\'is_itd\\'] == 1):,} synthetic ITD samples\")\\nprint(f\"\\nFeatures used:\\n{list(X.columns)}\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, prepare the data by separating features and target\n",
    "\n",
    "X, y = df.drop(\"is_itd\", axis=1), df[\"is_itd\"]\n",
    "\n",
    "# Drop non-numeric columns\n",
    "for col in [\"attack_state\"]:\n",
    "    if col in X.columns:\n",
    "        X = X.drop(col, axis=1)\n",
    "\n",
    "# Encode categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "for col in [\"protocol\", \"tcp_flags\", \"service\", \"time_period\",\"source_ip\",\"dest_ip\",\"source_port\",\"dest_port\"]:\n",
    "    if col in X.columns:\n",
    "        X[col] = encoder.fit_transform(X[col])\n",
    "'''\n",
    "# Check class distribution before SMOTE\n",
    "from collections import Counter\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(f\"Total samples: {len(y):,}\")\n",
    "print(f\"Class 0 (Normal): {sum(y == 0):,}\")\n",
    "print(f\"Class 1 (ITD): {sum(y == 1):,}\")\n",
    "print(f\"Imbalance ratio: {sum(y == 0) / sum(y == 1):.2f}:1\")\n",
    "print(f\"\\nTarget distribution:\\n{Counter(y)}\")\n",
    "\n",
    "# Apply SMOTE to upsample the minority class to 2:1 ratio (Normal:ITD)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"\\nApplying SMOTE upsampling to 2:1 ratio (Normal:ITD)...\")\n",
    "\n",
    "# Calculate target ratio: we want Class 0 : Class 1 = 2:1\n",
    "# So Class 1 should be Class 0 / 2\n",
    "minority_target = sum(y == 0) // 10\n",
    "\n",
    "smote = SMOTE(random_state=42, sampling_strategy={1: minority_target}, k_neighbors=5)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(f\"Total samples: {len(y_resampled):,}\")\n",
    "print(f\"Class 0 (Normal): {sum(y_resampled == 0):,}\")\n",
    "print(f\"Class 1 (ITD): {sum(y_resampled == 1):,}\")\n",
    "print(f\"New ratio: {sum(y_resampled == 0) / sum(y_resampled == 1):.2f}:1\")\n",
    "print(f\"\\nTarget distribution:\\n{Counter(y_resampled)}\")\n",
    "\n",
    "# Calculate expected NIR after SMOTE\n",
    "expected_nir = sum(y_resampled == 0) / len(y_resampled)\n",
    "print(f\"\\nExpected No Information Rate (NIR) after SMOTE: {expected_nir:.4f}\")\n",
    "\n",
    "# Update X and y to use resampled data\n",
    "X = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y = pd.Series(y_resampled, name='is_itd')\n",
    "\n",
    "print(\"\\nSMOTE upsampling completed successfully!\")\n",
    "print(f\"Generated {sum(y_resampled == 1) - sum(df['is_itd'] == 1):,} synthetic ITD samples\")\n",
    "print(f\"\\nFeatures used:\\n{list(X.columns)}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d9d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>duration</th>\n",
       "      <th>packets</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_per_packet</th>\n",
       "      <th>tcp_flags</th>\n",
       "      <th>packet_size_variance</th>\n",
       "      <th>connection_frequency</th>\n",
       "      <th>off_hours</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>is_internal_source</th>\n",
       "      <th>is_internal_dest</th>\n",
       "      <th>time_period</th>\n",
       "      <th>bytes_percentile</th>\n",
       "      <th>packets_percentile</th>\n",
       "      <th>duration_percentile</th>\n",
       "      <th>bpp_percentile</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5564</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12867</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25029</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9325</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dest_port  protocol  duration  packets  bytes  bytes_per_packet  tcp_flags  \\\n",
       "0       5564         2  0.026033        1      1               1.0          5   \n",
       "1      12867         1  0.026929        1      1               1.0          4   \n",
       "2      25029         0  0.017191        1      1               1.0          2   \n",
       "3      23032         1  0.014293        1      1               1.0          2   \n",
       "4       9325         1  0.024585        1      1               1.0          1   \n",
       "\n",
       "   packet_size_variance  connection_frequency  off_hours is_weekend  \\\n",
       "0                   0.0                     1          1      False   \n",
       "1                   0.0                     1          1      False   \n",
       "2                   0.0                     1          1      False   \n",
       "3                   0.0                     1          1      False   \n",
       "4                   0.0                     1          1      False   \n",
       "\n",
       "   hour_of_day  is_internal_source  is_internal_dest  time_period  \\\n",
       "0            7                   0                 1            1   \n",
       "1            7                   0                 1            1   \n",
       "2           21                   0                 1            5   \n",
       "3           20                   0                 1            5   \n",
       "4            8                   0                 1            1   \n",
       "\n",
       "   bytes_percentile  packets_percentile  duration_percentile  bpp_percentile  \\\n",
       "0               0.0                 0.0             0.236601             0.0   \n",
       "1               0.0                 0.0             0.248161             0.0   \n",
       "2               0.0                 0.0             0.128801             0.0   \n",
       "3               0.0                 0.0             0.092340             0.0   \n",
       "4               0.0                 0.0             0.218041             0.0   \n",
       "\n",
       "   service  \n",
       "0       15  \n",
       "1       15  \n",
       "2       15  \n",
       "3       15  \n",
       "4       15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb17534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: is_itd, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a142ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "for col in [\"protocol\", \"tcp_flags\", \"service\", \"time_period\",\"is_weekend\"]:\n",
    "    X[col] = encoder.fit_transform(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b757e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>duration</th>\n",
       "      <th>packets</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_per_packet</th>\n",
       "      <th>tcp_flags</th>\n",
       "      <th>packet_size_variance</th>\n",
       "      <th>connection_frequency</th>\n",
       "      <th>off_hours</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>is_internal_source</th>\n",
       "      <th>is_internal_dest</th>\n",
       "      <th>time_period</th>\n",
       "      <th>bytes_percentile</th>\n",
       "      <th>packets_percentile</th>\n",
       "      <th>duration_percentile</th>\n",
       "      <th>bpp_percentile</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5564</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12867</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25029</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9325</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dest_port  protocol  duration  packets  bytes  bytes_per_packet  tcp_flags  \\\n",
       "0       5564         2  0.026033        1      1               1.0          5   \n",
       "1      12867         1  0.026929        1      1               1.0          4   \n",
       "2      25029         0  0.017191        1      1               1.0          2   \n",
       "3      23032         1  0.014293        1      1               1.0          2   \n",
       "4       9325         1  0.024585        1      1               1.0          1   \n",
       "\n",
       "   packet_size_variance  connection_frequency  off_hours  is_weekend  \\\n",
       "0                   0.0                     1          1           0   \n",
       "1                   0.0                     1          1           0   \n",
       "2                   0.0                     1          1           0   \n",
       "3                   0.0                     1          1           0   \n",
       "4                   0.0                     1          1           0   \n",
       "\n",
       "   hour_of_day  is_internal_source  is_internal_dest  time_period  \\\n",
       "0            7                   0                 1            1   \n",
       "1            7                   0                 1            1   \n",
       "2           21                   0                 1            5   \n",
       "3           20                   0                 1            5   \n",
       "4            8                   0                 1            1   \n",
       "\n",
       "   bytes_percentile  packets_percentile  duration_percentile  bpp_percentile  \\\n",
       "0               0.0                 0.0             0.236601             0.0   \n",
       "1               0.0                 0.0             0.248161             0.0   \n",
       "2               0.0                 0.0             0.128801             0.0   \n",
       "3               0.0                 0.0             0.092340             0.0   \n",
       "4               0.0                 0.0             0.218041             0.0   \n",
       "\n",
       "   service  \n",
       "0       15  \n",
       "1       15  \n",
       "2       15  \n",
       "3       15  \n",
       "4       15  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e39b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (4999980, 20)\n",
      "Target distribution:\n",
      "is_itd\n",
      "0    4998141\n",
      "1       1839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Features used:\n",
      "['dest_port', 'protocol', 'duration', 'packets', 'bytes', 'bytes_per_packet', 'tcp_flags', 'packet_size_variance', 'connection_frequency', 'off_hours', 'is_weekend', 'hour_of_day', 'is_internal_source', 'is_internal_dest', 'time_period', 'bytes_percentile', 'packets_percentile', 'duration_percentile', 'bpp_percentile', 'service']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    classification_report, precision_score, recall_score\n",
    ")\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "# Display feature information\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeatures used:\\n{list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a7b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7698eb",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=== Grid Search for SMOTE Minority Target Ratios ===\\n\")\n",
    "\n",
    "# Compute scale_pos_weight for extreme imbalance\n",
    "neg_count = sum(y == 0)\n",
    "pos_count = sum(y == 1)\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "# Define ratio range to test (Normal:Phishing ratios from 5:1 to 30:1)\n",
    "ratio_range = range(5, 31,2)  # Test ratios 5, 6, 7, ..., 30\n",
    "\n",
    "# Stratified K-Fold CV\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for grid search results\n",
    "grid_results = []\n",
    "\n",
    "# Grid search over different SMOTE ratios\n",
    "for ratio in ratio_range:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Ratio: {ratio}:1 (Normal:ITD)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Storage for metrics for this ratio\n",
    "    acc_scores, roc_auc_scores, mcc_scores, kappa_scores, nir_scores= [], [], [], [], []\n",
    "    f1_scores, sensitivity_scores, specificity_scores = [], [], []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Calculate minority target based on current ratio\n",
    "        minority_target = sum(y_train == 0) // ratio\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42, sampling_strategy={1: minority_target}, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Update training data\n",
    "        X_train = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        y_train = pd.Series(y_resampled, name='is_itd')\n",
    "        \n",
    "        # Initialize XGBoost\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=200,\n",
    "            device=\"cuda\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            min_child_weight=1,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            eval_metric=\"aucpr\",\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        xgb_model.fit(X_train, y_train, verbose=False)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        most_freq_class = y_test.mode()[0]\n",
    "        nir = (y_test == most_freq_class).mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        acc_scores.append(acc)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        mcc_scores.append(mcc)\n",
    "        kappa_scores.append(kappa)\n",
    "        f1_scores.append(f1)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        specificity_scores.append(specificity)\n",
    "        nir_scores.append(nir)\n",
    "\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # Calculate means for this ratio\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_sens = np.mean(sensitivity_scores)\n",
    "    mean_spec = np.mean(specificity_scores)\n",
    "    mean_roc_auc = np.mean(roc_auc_scores)\n",
    "    mean_mcc = np.mean(mcc_scores)\n",
    "    mean_kappa = np.mean(kappa_scores)\n",
    "    mean_nir = np.mean(nir_scores)\n",
    "\n",
    "    \n",
    "    # Store results\n",
    "    grid_results.append({\n",
    "        'ratio': f\"{ratio}:1\",\n",
    "        'ratio_value': ratio,\n",
    "        'accuracy': mean_acc,\n",
    "        'f1_score': mean_f1,\n",
    "        'sensitivity': mean_sens,\n",
    "        'specificity': mean_spec,\n",
    "        'roc_auc': mean_roc_auc,\n",
    "        'mcc': mean_mcc,\n",
    "        'kappa': mean_kappa,\n",
    "        'nir': mean_nir\n",
    "\n",
    "    })\n",
    "    \n",
    "    # Print summary for this ratio\n",
    "    print(f\"\\nRatio {ratio}:1 - Mean Metrics:\")\n",
    "    print(f\"  Accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"  F1-score: {mean_f1:.4f}\")\n",
    "    print(f\"  Sensitivity: {mean_sens:.4f}\")\n",
    "    print(f\"  Specificity: {mean_spec:.4f}\")\n",
    "    print(f\"  ROC-AUC: {mean_roc_auc:.4f}\")\n",
    "    print(f\"  MCC: {mean_mcc:.4f}\")\n",
    "    print(f\"  Kappa: {mean_kappa:.4f}\")\n",
    "    print(f\"  NIR: {mean_nir:.4f}\")\n",
    "\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(grid_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== GRID SEARCH RESULTS SUMMARY ===\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best ratio for each metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== BEST RATIOS BY METRIC ===\")\n",
    "print(\"=\"*80)\n",
    "best_acc = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "best_mcc = results_df.loc[results_df['mcc'].idxmax()]\n",
    "best_roc = results_df.loc[results_df['roc_auc'].idxmax()]\n",
    "\n",
    "print(f\"Best Accuracy:   {best_acc['ratio']} (Acc={best_acc['accuracy']:.4f})\")\n",
    "print(f\"Best F1-score:   {best_f1['ratio']} (F1={best_f1['f1_score']:.4f})\")\n",
    "print(f\"Best MCC:        {best_mcc['ratio']} (MCC={best_mcc['mcc']:.4f})\")\n",
    "print(f\"Best ROC-AUC:    {best_roc['ratio']} (ROC-AUC={best_roc['roc_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de18298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimized XGBoost for Extreme Class Imbalance ===\n",
      "\n",
      "Class distribution:\n",
      "  Normal (0): 4,998,141\n",
      "  ITD (1): 1,839\n",
      "  Imbalance ratio: 2717.9:1\n",
      "  Base contamination: 0.000368\n",
      "\n",
      "Scale_pos_weight: 2717.9\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 10:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,398,363 samples (3,998,512 Normal, 399,851 ITD)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/xgboost/core.py:729: UserWarning: [10:17:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold: 0.9996 | F1: 0.2708 | PR-AUC: 0.1572 | Sens: 0.7657\n",
      "  Fold 2: Training on 4,398,364 samples (3,998,513 Normal, 399,851 ITD)\n",
      "    Threshold: 0.9988 | F1: 0.2819 | PR-AUC: 0.1756 | Sens: 0.8859\n",
      "  Fold 3: Training on 4,398,364 samples (3,998,513 Normal, 399,851 ITD)\n",
      "    Threshold: 0.9979 | F1: 0.2681 | PR-AUC: 0.1519 | Sens: 0.8886\n",
      "  Fold 4: Training on 4,398,364 samples (3,998,513 Normal, 399,851 ITD)\n",
      "    Threshold: 0.9997 | F1: 0.2751 | PR-AUC: 0.1863 | Sens: 0.7663\n",
      "  Fold 5: Training on 4,398,364 samples (3,998,513 Normal, 399,851 ITD)\n",
      "    Threshold: 0.9990 | F1: 0.2812 | PR-AUC: 0.1706 | Sens: 0.8995\n",
      "\n",
      "  Ratio 10:1 - Mean Metrics:\n",
      "    Accuracy:    0.9984 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1649\n",
      "    Recall:      0.8412\n",
      "    F1-score:    0.2754\n",
      "    F2-score:    0.4613\n",
      "    Specificity: 0.9984\n",
      "    ROC-AUC:     0.9990\n",
      "    PR-AUC:      0.1683 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3717\n",
      "    Kappa:       0.2750\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 20:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,198,437 samples (3,998,512 Normal, 199,925 ITD)\n",
      "    Threshold: 0.9870 | F1: 0.2689 | PR-AUC: 0.1572 | Sens: 0.9482\n",
      "  Fold 2: Training on 4,198,438 samples (3,998,513 Normal, 199,925 ITD)\n",
      "    Threshold: 0.9993 | F1: 0.2888 | PR-AUC: 0.1764 | Sens: 0.8261\n",
      "  Fold 3: Training on 4,198,438 samples (3,998,513 Normal, 199,925 ITD)\n",
      "    Threshold: 0.9953 | F1: 0.2683 | PR-AUC: 0.1578 | Sens: 0.9185\n",
      "  Fold 4: Training on 4,198,438 samples (3,998,513 Normal, 199,925 ITD)\n",
      "    Threshold: 0.9968 | F1: 0.2699 | PR-AUC: 0.1830 | Sens: 0.9076\n",
      "  Fold 5: Training on 4,198,438 samples (3,998,513 Normal, 199,925 ITD)\n",
      "    Threshold: 0.9996 | F1: 0.2804 | PR-AUC: 0.1649 | Sens: 0.7853\n",
      "\n",
      "  Ratio 20:1 - Mean Metrics:\n",
      "    Accuracy:    0.9983 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1636\n",
      "    Recall:      0.8771\n",
      "    F1-score:    0.2753\n",
      "    F2-score:    0.4670\n",
      "    Specificity: 0.9983\n",
      "    ROC-AUC:     0.9990\n",
      "    PR-AUC:      0.1679 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3778\n",
      "    Kappa:       0.2748\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 30:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,131,795 samples (3,998,512 Normal, 133,283 ITD)\n",
      "    Threshold: 0.9834 | F1: 0.2771 | PR-AUC: 0.1588 | Sens: 0.9510\n",
      "  Fold 2: Training on 4,131,796 samples (3,998,513 Normal, 133,283 ITD)\n",
      "    Threshold: 0.9937 | F1: 0.2922 | PR-AUC: 0.1777 | Sens: 0.9674\n",
      "  Fold 3: Training on 4,131,796 samples (3,998,513 Normal, 133,283 ITD)\n",
      "    Threshold: 0.9940 | F1: 0.2780 | PR-AUC: 0.1592 | Sens: 0.9375\n",
      "  Fold 4: Training on 4,131,796 samples (3,998,513 Normal, 133,283 ITD)\n",
      "    Threshold: 0.9856 | F1: 0.2766 | PR-AUC: 0.1802 | Sens: 0.9457\n",
      "  Fold 5: Training on 4,131,796 samples (3,998,513 Normal, 133,283 ITD)\n",
      "    Threshold: 0.9973 | F1: 0.2792 | PR-AUC: 0.1721 | Sens: 0.8995\n",
      "\n",
      "  Ratio 30:1 - Mean Metrics:\n",
      "    Accuracy:    0.9982 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1649\n",
      "    Recall:      0.9402\n",
      "    F1-score:    0.2806\n",
      "    F2-score:    0.4845\n",
      "    Specificity: 0.9982\n",
      "    ROC-AUC:     0.9990\n",
      "    PR-AUC:      0.1696 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3934\n",
      "    Kappa:       0.2802\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 40:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,098,474 samples (3,998,512 Normal, 99,962 ITD)\n",
      "    Threshold: 0.9508 | F1: 0.2780 | PR-AUC: 0.1583 | Sens: 0.9755\n",
      "  Fold 2: Training on 4,098,475 samples (3,998,513 Normal, 99,962 ITD)\n",
      "    Threshold: 0.9955 | F1: 0.2919 | PR-AUC: 0.1784 | Sens: 0.9538\n",
      "  Fold 3: Training on 4,098,475 samples (3,998,513 Normal, 99,962 ITD)\n",
      "    Threshold: 0.9948 | F1: 0.2770 | PR-AUC: 0.1576 | Sens: 0.9375\n",
      "  Fold 4: Training on 4,098,475 samples (3,998,513 Normal, 99,962 ITD)\n",
      "    Threshold: 0.9707 | F1: 0.2773 | PR-AUC: 0.1822 | Sens: 0.9620\n",
      "  Fold 5: Training on 4,098,475 samples (3,998,513 Normal, 99,962 ITD)\n",
      "    Threshold: 0.9977 | F1: 0.2830 | PR-AUC: 0.1726 | Sens: 0.9049\n",
      "\n",
      "  Ratio 40:1 - Mean Metrics:\n",
      "    Accuracy:    0.9982 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1653\n",
      "    Recall:      0.9467\n",
      "    F1-score:    0.2814\n",
      "    F2-score:    0.4865\n",
      "    Specificity: 0.9982\n",
      "    ROC-AUC:     0.9991\n",
      "    PR-AUC:      0.1698 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3952\n",
      "    Kappa:       0.2810\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 50:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,078,482 samples (3,998,512 Normal, 79,970 ITD)\n",
      "    Threshold: 0.9773 | F1: 0.2797 | PR-AUC: 0.1611 | Sens: 0.9700\n",
      "  Fold 2: Training on 4,078,483 samples (3,998,513 Normal, 79,970 ITD)\n",
      "    Threshold: 0.9991 | F1: 0.2951 | PR-AUC: 0.1790 | Sens: 0.8207\n",
      "  Fold 3: Training on 4,078,483 samples (3,998,513 Normal, 79,970 ITD)\n",
      "    Threshold: 0.9936 | F1: 0.2755 | PR-AUC: 0.1565 | Sens: 0.9348\n",
      "  Fold 4: Training on 4,078,483 samples (3,998,513 Normal, 79,970 ITD)\n",
      "    Threshold: 0.9630 | F1: 0.2778 | PR-AUC: 0.1800 | Sens: 0.9647\n",
      "  Fold 5: Training on 4,078,483 samples (3,998,513 Normal, 79,970 ITD)\n",
      "    Threshold: 0.9992 | F1: 0.2828 | PR-AUC: 0.1732 | Sens: 0.8043\n",
      "\n",
      "  Ratio 50:1 - Mean Metrics:\n",
      "    Accuracy:    0.9983 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1677\n",
      "    Recall:      0.8989\n",
      "    F1-score:    0.2822\n",
      "    F2-score:    0.4786\n",
      "    Specificity: 0.9983\n",
      "    ROC-AUC:     0.9991\n",
      "    PR-AUC:      0.1699 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3872\n",
      "    Kappa:       0.2817\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 75:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,051,825 samples (3,998,512 Normal, 53,313 ITD)\n",
      "    Threshold: 0.9774 | F1: 0.2719 | PR-AUC: 0.1591 | Sens: 0.9591\n",
      "  Fold 2: Training on 4,051,826 samples (3,998,513 Normal, 53,313 ITD)\n",
      "    Threshold: 0.9990 | F1: 0.2902 | PR-AUC: 0.1797 | Sens: 0.8125\n",
      "  Fold 3: Training on 4,051,826 samples (3,998,513 Normal, 53,313 ITD)\n",
      "    Threshold: 0.9842 | F1: 0.2711 | PR-AUC: 0.1573 | Sens: 0.9620\n",
      "  Fold 4: Training on 4,051,826 samples (3,998,513 Normal, 53,313 ITD)\n",
      "    Threshold: 0.9994 | F1: 0.2759 | PR-AUC: 0.1838 | Sens: 0.6793\n",
      "  Fold 5: Training on 4,051,826 samples (3,998,513 Normal, 53,313 ITD)\n",
      "    Threshold: 0.9984 | F1: 0.2802 | PR-AUC: 0.1723 | Sens: 0.8641\n",
      "\n",
      "  Ratio 75:1 - Mean Metrics:\n",
      "    Accuracy:    0.9984 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1666\n",
      "    Recall:      0.8554\n",
      "    F1-score:    0.2778\n",
      "    F2-score:    0.4651\n",
      "    Specificity: 0.9984\n",
      "    ROC-AUC:     0.9990\n",
      "    PR-AUC:      0.1705 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3758\n",
      "    Kappa:       0.2774\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 100:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,038,497 samples (3,998,512 Normal, 39,985 ITD)\n",
      "    Threshold: 0.9870 | F1: 0.2774 | PR-AUC: 0.1592 | Sens: 0.9591\n",
      "  Fold 2: Training on 4,038,498 samples (3,998,513 Normal, 39,985 ITD)\n",
      "    Threshold: 0.9989 | F1: 0.2896 | PR-AUC: 0.1781 | Sens: 0.8179\n",
      "  Fold 3: Training on 4,038,498 samples (3,998,513 Normal, 39,985 ITD)\n",
      "    Threshold: 0.9942 | F1: 0.2764 | PR-AUC: 0.1537 | Sens: 0.9375\n",
      "  Fold 4: Training on 4,038,498 samples (3,998,513 Normal, 39,985 ITD)\n",
      "    Threshold: 0.9995 | F1: 0.2752 | PR-AUC: 0.1849 | Sens: 0.5978\n",
      "  Fold 5: Training on 4,038,498 samples (3,998,513 Normal, 39,985 ITD)\n",
      "    Threshold: 0.9982 | F1: 0.2816 | PR-AUC: 0.1713 | Sens: 0.8832\n",
      "\n",
      "  Ratio 100:1 - Mean Metrics:\n",
      "    Accuracy:    0.9984 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1693\n",
      "    Recall:      0.8391\n",
      "    F1-score:    0.2800\n",
      "    F2-score:    0.4638\n",
      "    Specificity: 0.9985\n",
      "    ROC-AUC:     0.9990\n",
      "    PR-AUC:      0.1694 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3746\n",
      "    Kappa:       0.2796\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 150:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,025,168 samples (3,998,512 Normal, 26,656 ITD)\n",
      "    Threshold: 0.9808 | F1: 0.2815 | PR-AUC: 0.1603 | Sens: 0.9728\n",
      "  Fold 2: Training on 4,025,169 samples (3,998,513 Normal, 26,656 ITD)\n",
      "    Threshold: 0.9985 | F1: 0.2941 | PR-AUC: 0.1768 | Sens: 0.8315\n",
      "  Fold 3: Training on 4,025,169 samples (3,998,513 Normal, 26,656 ITD)\n",
      "    Threshold: 0.9934 | F1: 0.2768 | PR-AUC: 0.1541 | Sens: 0.9429\n",
      "  Fold 4: Training on 4,025,169 samples (3,998,513 Normal, 26,656 ITD)\n",
      "    Threshold: 0.9954 | F1: 0.2782 | PR-AUC: 0.1879 | Sens: 0.9212\n",
      "  Fold 5: Training on 4,025,169 samples (3,998,513 Normal, 26,656 ITD)\n",
      "    Threshold: 0.9991 | F1: 0.2811 | PR-AUC: 0.1724 | Sens: 0.7364\n",
      "\n",
      "  Ratio 150:1 - Mean Metrics:\n",
      "    Accuracy:    0.9984 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1686\n",
      "    Recall:      0.8810\n",
      "    F1-score:    0.2824\n",
      "    F2-score:    0.4754\n",
      "    Specificity: 0.9984\n",
      "    ROC-AUC:     0.9991\n",
      "    PR-AUC:      0.1703 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3842\n",
      "    Kappa:       0.2819\n",
      "\n",
      "======================================================================\n",
      "Testing Ratio: 200:1 (Normal:ITD)\n",
      "======================================================================\n",
      "  Fold 1: Training on 4,018,504 samples (3,998,512 Normal, 19,992 ITD)\n",
      "    Threshold: 0.9867 | F1: 0.2795 | PR-AUC: 0.1633 | Sens: 0.9646\n",
      "  Fold 2: Training on 4,018,505 samples (3,998,513 Normal, 19,992 ITD)\n",
      "    Threshold: 0.9953 | F1: 0.2918 | PR-AUC: 0.1780 | Sens: 0.9674\n",
      "  Fold 3: Training on 4,018,505 samples (3,998,513 Normal, 19,992 ITD)\n",
      "    Threshold: 0.9955 | F1: 0.2800 | PR-AUC: 0.1552 | Sens: 0.9375\n",
      "  Fold 4: Training on 4,018,505 samples (3,998,513 Normal, 19,992 ITD)\n",
      "    Threshold: 0.9989 | F1: 0.2845 | PR-AUC: 0.1807 | Sens: 0.7446\n",
      "  Fold 5: Training on 4,018,505 samples (3,998,513 Normal, 19,992 ITD)\n",
      "    Threshold: 0.9977 | F1: 0.2831 | PR-AUC: 0.1764 | Sens: 0.8777\n",
      "\n",
      "  Ratio 200:1 - Mean Metrics:\n",
      "    Accuracy:    0.9983 (NIR: 0.9996) ✗ Below NIR\n",
      "    Precision:   0.1689\n",
      "    Recall:      0.8984\n",
      "    F1-score:    0.2838\n",
      "    F2-score:    0.4804\n",
      "    Specificity: 0.9984\n",
      "    ROC-AUC:     0.9991\n",
      "    PR-AUC:      0.1707 ⭐ (Key metric for imbalanced data)\n",
      "    MCC:         0.3885\n",
      "    Kappa:       0.2833\n",
      "\n",
      "====================================================================================================\n",
      "=== GRID SEARCH RESULTS SUMMARY ===\n",
      "====================================================================================================\n",
      "ratio  ratio_value  accuracy  precision   recall  f1_score  f2_score  specificity  roc_auc   pr_auc      mcc    kappa      nir beats_nir\n",
      " 10:1           10  0.998373   0.164870 0.841177  0.275443  0.461306     0.998430 0.999003 0.168330 0.371676 0.274998 0.999632        NO\n",
      " 20:1           20  0.998295   0.163597 0.877146  0.275274  0.466969     0.998340 0.999018 0.167861 0.377781 0.274825 0.999632        NO\n",
      " 30:1           30  0.998227   0.164940 0.940191  0.280619  0.484537     0.998248 0.999015 0.169605 0.393366 0.280169 0.999632        NO\n",
      " 40:1           40  0.998221   0.165331 0.946726  0.281441  0.486484     0.998240 0.999065 0.169817 0.395154 0.280990 0.999632        NO\n",
      " 50:1           50  0.998313   0.167734 0.898897  0.282175  0.478567     0.998350 0.999061 0.169948 0.387218 0.281730 0.999632        NO\n",
      " 75:1           75  0.998360   0.166620 0.855413  0.277848  0.465111     0.998413 0.999038 0.170458 0.375837 0.277404 0.999632        NO\n",
      "100:1          100  0.998413   0.169285 0.839108  0.280038  0.463771     0.998472 0.999037 0.169436 0.374609 0.279599 0.999632        NO\n",
      "150:1          150  0.998350   0.168603 0.880963  0.282358  0.475444     0.998393 0.999058 0.170315 0.384167 0.281916 0.999632        NO\n",
      "200:1          200  0.998332   0.168894 0.898350  0.283792  0.480379     0.998369 0.999068 0.170707 0.388464 0.283349 0.999632        NO\n",
      "\n",
      "====================================================================================================\n",
      "=== BEST RATIOS BY METRIC ===\n",
      "====================================================================================================\n",
      "Best Accuracy:   100:1 (Acc=0.9984, Beats NIR: NO)\n",
      "Best F1-score:   200:1 (F1=0.2838)\n",
      "Best F2-score:   40:1 (F2=0.4865)\n",
      "Best PR-AUC:     200:1 (PR-AUC=0.1707) ⭐ RECOMMENDED\n",
      "Best MCC:        40:1 (MCC=0.3952)\n",
      "\n",
      "====================================================================================================\n",
      "=== KEY INSIGHTS ===\n",
      "====================================================================================================\n",
      "1. Focus on PR-AUC (Precision-Recall AUC) - it's better than ROC-AUC for extreme imbalance\n",
      "2. Optimal threshold found per fold using precision-recall curve\n",
      "3. F2-score emphasizes recall (catching threats) over precision\n",
      "4. Models that beat NIR (0.9996): 0/9\n",
      "\n",
      "Recommended ratio for deployment: 200:1 (highest PR-AUC)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=== Optimized XGBoost for Extreme Class Imbalance ===\\n\")\n",
    "\n",
    "# Compute scale_pos_weight for extreme imbalance\n",
    "neg_count = sum(y == 0)\n",
    "pos_count = sum(y == 1)\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Normal (0): {neg_count:,}\")\n",
    "print(f\"  ITD (1): {pos_count:,}\")\n",
    "print(f\"  Imbalance ratio: {scale_pos_weight:.1f}:1\")\n",
    "print(f\"  Base contamination: {pos_count/len(y):.6f}\")\n",
    "print(f\"\\nScale_pos_weight: {scale_pos_weight:.1f}\\n\")\n",
    "\n",
    "# Define ratio range - focus on ranges that make sense for extreme imbalance\n",
    "# Test from 10:1 up to 100:1 to find sweet spot\n",
    "ratio_range = [10, 20, 30, 40, 50, 75, 100, 150, 200]\n",
    "\n",
    "# Stratified K-Fold CV\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for grid search results\n",
    "grid_results = []\n",
    "\n",
    "# Grid search over different SMOTE ratios\n",
    "for ratio in ratio_range:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Ratio: {ratio}:1 (Normal:ITD)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Storage for metrics for this ratio\n",
    "    acc_scores, roc_auc_scores, pr_auc_scores, mcc_scores, kappa_scores, nir_scores = [], [], [], [], [], []\n",
    "    f1_scores, f2_scores, sensitivity_scores, specificity_scores, precision_scores = [], [], [], [], []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Calculate minority target based on current ratio\n",
    "        minority_target = sum(y_train == 0) // ratio\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42, sampling_strategy={1: minority_target}, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Update training data\n",
    "        X_train = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        y_train = pd.Series(y_resampled, name='is_itd')\n",
    "        \n",
    "        print(f\"  Fold {fold}: Training on {len(X_train):,} samples ({sum(y_train==0):,} Normal, {sum(y_train==1):,} ITD)\")\n",
    "        \n",
    "        # Initialize XGBoost with optimized parameters for imbalanced data\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=300,  # More trees for better learning\n",
    "            device=\"cuda\",\n",
    "            learning_rate=0.05,  # Lower learning rate for better convergence\n",
    "            max_depth=4,  # Slightly deeper for more complex patterns\n",
    "            subsample=0.8,\n",
    "            min_child_weight=1,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,  # Critical for imbalance\n",
    "            eval_metric=\"aucpr\",  # PR-AUC better for imbalanced data\n",
    "            gamma=0.1,  # Regularization to prevent overfitting\n",
    "            reg_alpha=0.1,  # L1 regularization\n",
    "            reg_lambda=1.0,  # L2 regularization\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit model\n",
    "        xgb_model.fit(X_train, y_train, verbose=False)\n",
    "        \n",
    "        # Predict probabilities\n",
    "        y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Find optimal threshold using precision-recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "        f1_scores_at_thresholds = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        optimal_idx = np.argmax(f1_scores_at_thresholds)\n",
    "        optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "        \n",
    "        # Predict using optimal threshold\n",
    "        y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        pr_auc = average_precision_score(y_test, y_prob)  # Better than ROC-AUC for imbalanced\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # F2-score (weights recall 2x more than precision)\n",
    "        f2 = (5 * precision[optimal_idx] * recall[optimal_idx]) / (4 * precision[optimal_idx] + recall[optimal_idx] + 1e-10)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        most_freq_class = y_test.mode()[0]\n",
    "        nir = (y_test == most_freq_class).mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        acc_scores.append(acc)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        pr_auc_scores.append(pr_auc)\n",
    "        mcc_scores.append(mcc)\n",
    "        kappa_scores.append(kappa)\n",
    "        f1_scores.append(f1)\n",
    "        f2_scores.append(f2)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        specificity_scores.append(specificity)\n",
    "        precision_scores.append(prec)\n",
    "        nir_scores.append(nir)\n",
    "        \n",
    "        print(f\"    Threshold: {optimal_threshold:.4f} | F1: {f1:.4f} | PR-AUC: {pr_auc:.4f} | Sens: {sensitivity:.4f}\")\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # Calculate means for this ratio\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_f2 = np.mean(f2_scores)\n",
    "    mean_sens = np.mean(sensitivity_scores)\n",
    "    mean_spec = np.mean(specificity_scores)\n",
    "    mean_prec = np.mean(precision_scores)\n",
    "    mean_roc_auc = np.mean(roc_auc_scores)\n",
    "    mean_pr_auc = np.mean(pr_auc_scores)\n",
    "    mean_mcc = np.mean(mcc_scores)\n",
    "    mean_kappa = np.mean(kappa_scores)\n",
    "    mean_nir = np.mean(nir_scores)\n",
    "    \n",
    "    # Store results\n",
    "    grid_results.append({\n",
    "        'ratio': f\"{ratio}:1\",\n",
    "        'ratio_value': ratio,\n",
    "        'accuracy': mean_acc,\n",
    "        'precision': mean_prec,\n",
    "        'recall': mean_sens,\n",
    "        'f1_score': mean_f1,\n",
    "        'f2_score': mean_f2,\n",
    "        'specificity': mean_spec,\n",
    "        'roc_auc': mean_roc_auc,\n",
    "        'pr_auc': mean_pr_auc,\n",
    "        'mcc': mean_mcc,\n",
    "        'kappa': mean_kappa,\n",
    "        'nir': mean_nir,\n",
    "        'beats_nir': 'YES' if mean_acc > mean_nir else 'NO'\n",
    "    })\n",
    "    \n",
    "    # Print summary for this ratio\n",
    "    print(f\"\\n  Ratio {ratio}:1 - Mean Metrics:\")\n",
    "    print(f\"    Accuracy:    {mean_acc:.4f} (NIR: {mean_nir:.4f}) {'✓ BEATS NIR' if mean_acc > mean_nir else '✗ Below NIR'}\")\n",
    "    print(f\"    Precision:   {mean_prec:.4f}\")\n",
    "    print(f\"    Recall:      {mean_sens:.4f}\")\n",
    "    print(f\"    F1-score:    {mean_f1:.4f}\")\n",
    "    print(f\"    F2-score:    {mean_f2:.4f}\")\n",
    "    print(f\"    Specificity: {mean_spec:.4f}\")\n",
    "    print(f\"    ROC-AUC:     {mean_roc_auc:.4f}\")\n",
    "    print(f\"    PR-AUC:      {mean_pr_auc:.4f} ⭐ (Key metric for imbalanced data)\")\n",
    "    print(f\"    MCC:         {mean_mcc:.4f}\")\n",
    "    print(f\"    Kappa:       {mean_kappa:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(grid_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=== GRID SEARCH RESULTS SUMMARY ===\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best ratio for each metric\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=== BEST RATIOS BY METRIC ===\")\n",
    "print(\"=\"*100)\n",
    "best_acc = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "best_f2 = results_df.loc[results_df['f2_score'].idxmax()]\n",
    "best_pr_auc = results_df.loc[results_df['pr_auc'].idxmax()]\n",
    "best_mcc = results_df.loc[results_df['mcc'].idxmax()]\n",
    "\n",
    "print(f\"Best Accuracy:   {best_acc['ratio']} (Acc={best_acc['accuracy']:.4f}, Beats NIR: {best_acc['beats_nir']})\")\n",
    "print(f\"Best F1-score:   {best_f1['ratio']} (F1={best_f1['f1_score']:.4f})\")\n",
    "print(f\"Best F2-score:   {best_f2['ratio']} (F2={best_f2['f2_score']:.4f})\")\n",
    "print(f\"Best PR-AUC:     {best_pr_auc['ratio']} (PR-AUC={best_pr_auc['pr_auc']:.4f}) ⭐ RECOMMENDED\")\n",
    "print(f\"Best MCC:        {best_mcc['ratio']} (MCC={best_mcc['mcc']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"=== KEY INSIGHTS ===\")\n",
    "print(\"=\"*100)\n",
    "print(\"1. Focus on PR-AUC (Precision-Recall AUC) - it's better than ROC-AUC for extreme imbalance\")\n",
    "print(\"2. Optimal threshold found per fold using precision-recall curve\")\n",
    "print(\"3. F2-score emphasizes recall (catching threats) over precision\")\n",
    "print(f\"4. Models that beat NIR ({results_df['nir'].iloc[0]:.4f}): {sum(results_df['beats_nir'] == 'YES')}/{len(results_df)}\")\n",
    "print(f\"\\nRecommended ratio for deployment: {best_pr_auc['ratio']} (highest PR-AUC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5b7898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Grid Search for SMOTE Minority Target Ratios with Isolation Forest ===\n",
      "\n",
      "Original class distribution: Counter({0: 4998141, 1: 1839})\n",
      "Contamination rate: 0.000368\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing Ratio: 500:1 (Normal:ITD)\n",
      "============================================================\n",
      "\n",
      "Ratio 500:1 - Mean Metrics:\n",
      "  Accuracy: 0.9976\n",
      "  F1-score: 0.0000\n",
      "  Sensitivity: 0.0000\n",
      "  Specificity: 0.9980\n",
      "  ROC-AUC: 0.9580\n",
      "  MCC: -0.0009\n",
      "  Kappa: -0.0006\n",
      "  NIR: 0.9996\n",
      "\n",
      "============================================================\n",
      "Testing Ratio: 600:1 (Normal:ITD)\n",
      "============================================================\n",
      "\n",
      "Ratio 600:1 - Mean Metrics:\n",
      "  Accuracy: 0.9980\n",
      "  F1-score: 0.0000\n",
      "  Sensitivity: 0.0000\n",
      "  Specificity: 0.9983\n",
      "  ROC-AUC: 0.9591\n",
      "  MCC: -0.0008\n",
      "  Kappa: -0.0006\n",
      "  NIR: 0.9996\n",
      "\n",
      "============================================================\n",
      "Testing Ratio: 700:1 (Normal:ITD)\n",
      "============================================================\n",
      "\n",
      "Ratio 700:1 - Mean Metrics:\n",
      "  Accuracy: 0.9982\n",
      "  F1-score: 0.0000\n",
      "  Sensitivity: 0.0000\n",
      "  Specificity: 0.9986\n",
      "  ROC-AUC: 0.9589\n",
      "  MCC: -0.0007\n",
      "  Kappa: -0.0006\n",
      "  NIR: 0.9996\n",
      "\n",
      "============================================================\n",
      "Testing Ratio: 800:1 (Normal:ITD)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 72\u001b[0m\n\u001b[1;32m     62\u001b[0m iso_forest \u001b[38;5;241m=\u001b[39m IsolationForest(\n\u001b[1;32m     63\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     64\u001b[0m     contamination\u001b[38;5;241m=\u001b[39mtrain_contamination,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Fit model (Isolation Forest is unsupervised, but we use labels for evaluation)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43miso_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Predict (-1 for anomalies/outliers, 1 for inliers)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m y_pred_raw \u001b[38;5;241m=\u001b[39m iso_forest\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:381\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    380\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtocsr()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontamination)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:540\u001b[0m, in \u001b[0;36mIsolationForest._score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    537\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Take the opposite of the scores as bigger is better (here less abnormal)\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_chunked_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:570\u001b[0m, in \u001b[0;36mIsolationForest._compute_chunked_score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    566\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_samples, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# compute score on the slices of test samples:\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     scores[sl] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:606\u001b[0m, in \u001b[0;36mIsolationForest._compute_score_samples\u001b[0;34m(self, X, subsample_features)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Note: we use default n_jobs value, i.e. sequential computation, which\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# we expect to be more performant that parallelizing for small number\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# of samples, e.g. < 1k samples. Default n_jobs value can be overriden\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# https://github.com/scikit-learn/scikit-learn/pull/28622 for more\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# details.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m--> 606\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_compute_tree_depths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_path_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_average_path_length_per_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_features_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m*\u001b[39m average_path_length_max_samples\n\u001b[1;32m    625\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# For a single training sample, denominator and depth are 0.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Therefore, we set the score manually to 1.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    631\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/joblib/parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/joblib/parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ads_5984/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:46\u001b[0m, in \u001b[0;36m_parallel_compute_tree_depths\u001b[0;34m(tree, X, features, tree_decision_path_lengths, tree_avg_path_lengths, depths, lock)\u001b[0m\n\u001b[1;32m     42\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m X[:, features]\n\u001b[1;32m     44\u001b[0m leaves_index \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mapply(X_subset, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[1;32m     47\u001b[0m     depths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m         tree_decision_path_lengths[leaves_index]\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;241m+\u001b[39m tree_avg_path_lengths[leaves_index]\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=== Grid Search for SMOTE Minority Target Ratios with Isolation Forest ===\\n\")\n",
    "\n",
    "# Compute contamination parameter (expected proportion of outliers)\n",
    "pos_count = sum(y == 1)\n",
    "total_count = len(y)\n",
    "contamination_rate = pos_count / total_count\n",
    "\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "print(f\"Contamination rate: {contamination_rate:.6f}\\n\")\n",
    "\n",
    "# Define ratio range to test (Normal:ITD ratios from 5:1 to 30:1)\n",
    "ratio_range = range(500, 1000, 100)\n",
    "\n",
    "# Stratified K-Fold CV\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for grid search results\n",
    "grid_results = []\n",
    "\n",
    "# Grid search over different SMOTE ratios\n",
    "for ratio in ratio_range:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Ratio: {ratio}:1 (Normal:ITD)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Storage for metrics for this ratio\n",
    "    acc_scores, roc_auc_scores, mcc_scores, kappa_scores, nir_scores = [], [], [], [], []\n",
    "    f1_scores, sensitivity_scores, specificity_scores = [], [], []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Calculate minority target based on current ratio\n",
    "        minority_target = sum(y_train == 0) // ratio\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42, sampling_strategy={1: minority_target}, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Update training data\n",
    "        X_train = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        y_train = pd.Series(y_resampled, name='is_itd')\n",
    "        \n",
    "        # Calculate contamination for this resampled training set\n",
    "        train_contamination = sum(y_train == 1) / len(y_train)\n",
    "        \n",
    "        # Initialize Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            n_estimators=200,\n",
    "            contamination=train_contamination,\n",
    "            max_samples='auto',\n",
    "            max_features=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit model (Isolation Forest is unsupervised, but we use labels for evaluation)\n",
    "        iso_forest.fit(X_train)\n",
    "        \n",
    "        # Predict (-1 for anomalies/outliers, 1 for inliers)\n",
    "        y_pred_raw = iso_forest.predict(X_test)\n",
    "        \n",
    "        # Convert to binary: -1 (anomaly) -> 1 (ITD), 1 (inlier) -> 0 (Normal)\n",
    "        y_pred = np.where(y_pred_raw == -1, 1, 0)\n",
    "        \n",
    "        # Get anomaly scores (more negative = more anomalous)\n",
    "        anomaly_scores = iso_forest.score_samples(X_test)\n",
    "        \n",
    "        # Convert to probability-like scores (0 to 1, where higher = more likely ITD)\n",
    "        y_prob = 1 - (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min())\n",
    "        \n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        most_freq_class = y_test.mode()[0]\n",
    "        nir = (y_test == most_freq_class).mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        acc_scores.append(acc)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        mcc_scores.append(mcc)\n",
    "        kappa_scores.append(kappa)\n",
    "        f1_scores.append(f1)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        specificity_scores.append(specificity)\n",
    "        nir_scores.append(nir)\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # Calculate means for this ratio\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_sens = np.mean(sensitivity_scores)\n",
    "    mean_spec = np.mean(specificity_scores)\n",
    "    mean_roc_auc = np.mean(roc_auc_scores)\n",
    "    mean_mcc = np.mean(mcc_scores)\n",
    "    mean_kappa = np.mean(kappa_scores)\n",
    "    mean_nir = np.mean(nir_scores)\n",
    "    \n",
    "    # Store results\n",
    "    grid_results.append({\n",
    "        'ratio': f\"{ratio}:1\",\n",
    "        'ratio_value': ratio,\n",
    "        'accuracy': mean_acc,\n",
    "        'f1_score': mean_f1,\n",
    "        'sensitivity': mean_sens,\n",
    "        'specificity': mean_spec,\n",
    "        'roc_auc': mean_roc_auc,\n",
    "        'mcc': mean_mcc,\n",
    "        'kappa': mean_kappa,\n",
    "        'nir': mean_nir\n",
    "    })\n",
    "    \n",
    "    # Print summary for this ratio\n",
    "    print(f\"\\nRatio {ratio}:1 - Mean Metrics:\")\n",
    "    print(f\"  Accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"  F1-score: {mean_f1:.4f}\")\n",
    "    print(f\"  Sensitivity: {mean_sens:.4f}\")\n",
    "    print(f\"  Specificity: {mean_spec:.4f}\")\n",
    "    print(f\"  ROC-AUC: {mean_roc_auc:.4f}\")\n",
    "    print(f\"  MCC: {mean_mcc:.4f}\")\n",
    "    print(f\"  Kappa: {mean_kappa:.4f}\")\n",
    "    print(f\"  NIR: {mean_nir:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(grid_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== GRID SEARCH RESULTS SUMMARY ===\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best ratio for each metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== BEST RATIOS BY METRIC ===\")\n",
    "print(\"=\"*80)\n",
    "best_acc = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "best_mcc = results_df.loc[results_df['mcc'].idxmax()]\n",
    "best_roc = results_df.loc[results_df['roc_auc'].idxmax()]\n",
    "\n",
    "print(f\"Best Accuracy:   {best_acc['ratio']} (Acc={best_acc['accuracy']:.4f})\")\n",
    "print(f\"Best F1-score:   {best_f1['ratio']} (F1={best_f1['f1_score']:.4f})\")\n",
    "print(f\"Best MCC:        {best_mcc['ratio']} (MCC={best_mcc['mcc']:.4f})\")\n",
    "print(f\"Best ROC-AUC:    {best_roc['ratio']} (ROC-AUC={best_roc['roc_auc']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc32a32e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Sum up confusion matrices from all folds\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m total_tn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m])\n\u001b[1;32m      6\u001b[0m total_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[1;32m      7\u001b[0m total_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Create aggregated confusion matrix from all folds (XGBoost)\n",
    "import seaborn as sns\n",
    "\n",
    "# Sum up confusion matrices from all folds\n",
    "total_tn = sum([r['tn'] for r in results])\n",
    "total_fp = sum([r['fp'] for r in results])\n",
    "total_fn = sum([r['fn'] for r in results])\n",
    "total_tp = sum([r['tp'] for r in results])\n",
    "\n",
    "# Create confusion matrix array\n",
    "cm_total = np.array([[total_tn, total_fp], \n",
    "                     [total_fn, total_tp]])\n",
    "\n",
    "# Visualize aggregated confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Normal', 'Insider Threat'],\n",
    "            yticklabels=['Normal', 'Insider Threat'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title('Aggregated Confusion Matrix - XGBoost Model (All Folds)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed confusion matrix breakdown\n",
    "print(\"\\n=== Aggregated Confusion Matrix (All Folds Combined) ===\")\n",
    "print(f\"True Negatives (TN): {total_tn:,}\")\n",
    "print(f\"False Positives (FP): {total_fp:,}\")\n",
    "print(f\"False Negatives (FN): {total_fn:,}\")\n",
    "print(f\"True Positives (TP): {total_tp:,}\")\n",
    "print(f\"\\nTotal Predictions: {total_tn + total_fp + total_fn + total_tp:,}\")\n",
    "print(f\"Overall ITD Detection Rate: {total_tp / (total_tp + total_fn) * 100:.2f}%\")\n",
    "print(f\"Overall False Alarm Rate: {total_fp / (total_fp + total_tn) * 100:.2f}%\")\n",
    "print(f\"Overall Precision: {total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0:.4f}\")\n",
    "print(f\"Overall Recall: {total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads_5984",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
